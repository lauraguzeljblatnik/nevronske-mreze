\documentclass[mat1]{fmfdelo}
% \documentclass[fin1]{fmfdelo}
% \documentclass[isrm1]{fmfdelo}
% \documentclass[mat2]{fmfdelo}
% \documentclass[fin2]{fmfdelo}
% \documentclass[isrm2]{fmfdelo}

% naslednje ukaze ustrezno napolnite
\avtor{Laura Guzelj Blatnik}

\naslov{Nevronske mreže z vzvratnim razširjanjem napak v funkcijskem programskem jeziku}
\title{Angleški prevod slovenskega naslova dela}

% navedite ime mentorja s polnim nazivom: doc.~dr.~Ime Priimek,
% izr.~prof.~dr.~Ime Priimek, prof.~dr.~Ime Priimek
% uporabite le tisti ukaz/ukaze, ki je/so za vas ustrezni
\mentor{prof.~dr.~Ljupčo Todorovski}
% \mentorica{}
 \somentor{asist.~dr.~Aljaž Osojnik}
% \somentorica{}
% \mentorja{}{}
% \mentorici{}{}

\letnica{2020} % leto diplome

%  V povzetku na kratko opišite vsebinske rezultate dela. Sem ne sodi razlaga organizacije dela --
%  v katerem poglavju/razdelku je kaj, pač pa le opis vsebine.
\povzetek{}

%  Prevod slovenskega povzetka v angleščino.
\abstract{}

% navedite vsaj eno klasifikacijsko oznako --
% dostopne so na www.ams.org/mathscinet/msc/msc2010.html
\klasifikacija{}
\kljucnebesede{} % navedite nekaj ključnih pojmov, ki nastopajo v delu
\keywords{} % angleški prevod ključnih besed

\zapisiMetaPodatke  % poskrbi za metapodatke in veljaven PDF/A-1b standard

% aktivirajte pakete, ki jih potrebujete
% \usepackage{tikz}

% za številske množice uporabite naslednje simbole
\newcommand{\R}{\mathbb R}
\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\C}{\mathbb C}
\newcommand{\Q}{\mathbb Q}

% matematične operatorje deklarirajte kot take, da jih bo Latex pravilno stavil
% \DeclareMathOperator{\conv}{conv}

% vstavite svoje definicije ...
%  \newcommand{}{}

\begin{document}

\section{Uvod}

Človeški možgani so kompleksen organ predvsem zaradi neštetih funkcij, ki jih opravljajo. In zgolj vprašanje časa je bilo, kdaj bodo znanstveniki skoraj neskončne zmožnosti možganov prenesli v računalništvo. Ko so poskušali idejo uresničiti, so si predvsem želeli strukture, ki se bo - podobno kot možgani - sposobna učiti, odzivati na sprememe in prepoznati neznane situacije. Tako je 1949 Donal Hebb v svojem delu prvič predstavil idejo umetnih nevronskih mrežah, kjer se je zgledoval predvsem po možganih. Dvoslojni perceptorn se je rodil nekoliko kasneje, leta 1962 je ta pojem prvi omenil Rosenblatt. Z razvojem storjne opreme se je zanimanje za nevronske mreže zopet povečalo v osemdestih lezih prejšnjega stoletja. Leta 1986 je več razskovalcev neodvisno drug od drugega vpelje teorijo o večslojnih nevronskih mrežah in vzvratnem razširjanju napake. Kljub širokemu spektru uporabe, ki jih nudijo nevronske mreže je interes zanje kasneje nekoliko upadel. V zadnjih letih pa so nevronske mreže zopet vroča tema, predvsem zaradi njihove zmožnosti prepoznavanja slik. Do danes so umetne nevronske mreže močno napredovale, namesto običajnih mrež se vedno bolj uporabljajo globoke nevronske mreže, ki v nekaterih lastnostih celo prekašajo možgane. Kljub vsemu pa so možgani sposobni masrcičesa, česar računalniki ne bodo nikoli.

Diplomska naloga se poglobi v umetne nevronske mreže z vzvratnim razširjanjem napake. Primer take mreže sem tudi implementirala v funkcijskem programskem jeziku OCaml. 

\section{Umetne nevronske mreže}

\subsection{Biološko ozadje}
Kot sugerira že samo ime, se umetne nevronske mreže v marsičem zgledujejo po človeških možganih. Osnovni gradniki možganov so nevroni, ki so med seboj povezani s sinapsami. Prav te goste povezave so ključne za delovanje živčevja. Skozi naše življenje se sinapse spreminjajo po jakosti in številu, ko se učimo se ustvarjajo nove povezave med nevroni, že obstoječe pa postajajo močnejše. Nevron se aktivira le,  ko po sinapsi do njega pride signal s točno določeno frekvenco, ta impulz nato nevron posreduje sosednjim nevronom. Človeške možgane sestavlja $10^{10}$ nevronov, vsak  ima približno $10^4$ sinaps. Umetne nevronske mreže načeloma sestavlja veliko manj nevronov, posledično so zato pri izvajanju operacij veliko hitrejše od možganov. Kot bomo videli v nadaljevanju, je osnovna ideja umetnih nevronskih mrež zelo podobna živčevju v možganih. Lahko bi rekli, da so umetne nevronske mreže zgolj abstraktna poenosatvitev delovanja možganov. 

%SLIKA NEVRONOV V MOŽGANIH?


\subsection{Zgradba nevronskih mrež}
Nevronske mreže so sestavljene iz nevronov, ki matematično gledano niso nič drugega kot funkcije. Nevroni so med seboj povezani, vsaka povezava ima svojo težo oziroma utež. Nevrone razdelimo v sloje, pomembno je poudariti, da nevroni v istem sloju med seboj niso povezani, vsak nevron je povezan le z vsemi nevroni v sosednjem (oziroma prejšnjem sloju). Nevronska mreža lahko vsebuje le vhodni in izhodni sloj - takrat govorimo o dvoslojni nevronski mreži. Če se med tema slojema skriva še kakšen skriti sloj, potem mreža postane večslojna. Število nevronov v posameznem sloju tako kot tudi število skritih slojev nevronske mreže je odvisno od problema, ki ga nevronska mreža rešuje.

%VSTAVI SLIKO NEVRONSKE MREŽE

Pri mrežah, ki jih bom obravnavala v diplomski nalogi, so vse povezave usmerjene, čeprav je pomembno omeniti, da obstajajo tudi mreže z dvosmernimi povezavami. Usmerjeni nevronski mreži rečemo perceptron, glede na število slojev pa ločimo dvoslojni in večslojni perceptron.  

\subsection{Nevron}

\subsection{Delovanje perceptrona}
Za začetek vzemimo dvoslojni perceptron z $m$ nevroni v vhodnem sloju, njihova stanja  označimo z $X_1, X_2, \ldots, X_m$ in $n$ nevroni v izhodnem sloju, označimo jih $Y_1, Y_2, \ldots, Y_{n}$. Običajno sloj nevronov zapišemo v obliki vektorja:
\[ X = (X_1, X_2, \dotso, X_m)^\intercal  \quad \text{in} \quad  Y = (Y_1, Y_2, \dotso, Y_n)^\intercal \text{.}\]

Uteži med nevroni predstavimo z matriko:

\[ M = \begin{bmatrix}
W_{11} &W_{12}  & \cdots & W_{1m}  \\
W_{21}  &W_{22}  &\cdots & W_{2m}  \\
\cdots &  &\cdots &  \\
W_{n1} & W_{n2} & \cdots & W_{nm} 
\end{bmatrix}  \]

\subsubsection{Delovanje nevrona}
Nevron prejme vhodno stanje, na njem uporabi funkcijo kombinacije in nato še funkcijo aktivacije.

V vsakem sloju nevroni neodvisno izračunajo svoje izhode in te vrenosti posredujejo nevronom v naslednjem sloju



\subsection{Lastnosti nevronskih mrež}
Nevronske mreže odlikujejo tako pozitivne kot negativne lastnosti. Med pozitivne spadajo:
\begin{enumerate}
\item paralelizabilnost:
\item robustnost:
\item 
\end{enumerate}
Glavna pomanjkljivost nevronskih mrež je določanje topologije mreže. Število nevronov v posameznem skritem sloju in število skritih slojev je odvisno od posameznega primera. Idealno topologijo nevronske mreže je tako težko doseči, do nje lahko pridemo le z ugibanjem. Še ena slaba lastnost pa je nezmožnost razlaganja odločitev. Zaradi kompleksne strukture je vsak rezultat mreže produkt večih neodvisnih operacij nevronov, ki jih ne moremo razložiti.


\subsection{Uporaba nevronskih mrež}
Zaradi razvoja strojne opreme v zadnjih letih so močno napredovale tudi nevronske mreže. Poleg tega so bile razvite tudi hitrjše implementacije algoritmov, posledično je uporaba nevronskih mrež danes zelo pogosta v industriji. Pomembno je tudi dejstvo, da nevronske mreže lahko rešujejo najrazličnejše probleme, zato se dandanes nevronske mreže porabljajo na skoraj vseh mogočih področjih. Dva najpomembenjša namena uporabe sta regresija in klasifikacija. Področja industrijske uporabe so: 
\begin{itemize}
\item prepoznavanje in klasifikacija slik: konkretno podjetje Google uporablja nevronske mreže za prepoznavanje slik, ki jih nato opremi s ključnimi besedami
\item prepoznavanje ročno napisanih besedil,
\item prepoznavanje  govora, tu bi veljalo izpostaviti podjete Miscrosoft, ki je razvilo nevronsko mrežo, ki govorjeno angleščino prevaja v kitajščino
\item kontrola kvalitete v produkciji,
\item v zdravstvu se uporabljajo za določanje pacientove diagnoze
\item napovedovanje različnih vrednosti glede na trenutne trende.

\end{itemize}


\section{Učenje}
Najprej je smiselno definirati, kaj pojem učenje sploh pomeni. Po [VSTAVI VIR] za učenje potrebujemo sistem, ki strmi k izpolnitvi določene naloge oziroma cilja. Pred učenjem sistem ni sposoben zadostno opraviti naloge. S ponavljanjem določenih opravil poskušamo sistem pripraviti do tega, da bo deloval bolje, kjer je bolje lahko hitreje, ceneje, bolj pravilno\ldots Učenje lahko torej definiramo kot zaporedje ponovitev, kjer pri vsaki ponovitvi skušamo zmanjšati napako tako, da bi se zastavljenemu cilju čimbolj približali. 

Obstaja kar nekaj različnih pravil, kako umetno nevronsko mrežo naučiti pravilnega delovanja. V svoji diplomski nalogi sem se osredotočila na posplošeno pravilo delta oziroma vzvratno razširjanje napake, ki je nekako standaren algoritem pri večslojnih perceptronih. 

\subsection{Pravilo delta}
Pravilo delta se uporablja pri učenju dvoslojnih perceptronov. Če je mreža večslojna, potem govorimo o vzvratnem razširjanju napake. Najprej si poglejmo kako deluje pravilo delta, saj je vzvratno razširjanje napake samo posplošitev pravila delta.

\subsection{Ideja vzvratnega razširjanja napake}
Učenje umetne nevronske mreže poteka tako, da iz začetnih poljubno izbranih uteži (pomembno je le, da niso vse uteži nastavljene na 0), uteži popravimo tako, da bo mreža iz vhodnih podatkov znala napovedati izhod. Da mrežo naučimo pravilenga delovanja potrebujemo učne primere - torej vhodne podatke in željene izhode za te vrednosti. 
Za vzvratnim razširjanjem napake stoji povsem preprosta ideja. Najprej uteži med nevroni poljubno nastavimo, nato na izbranem testnem primeru izračunamo za koliko se je naša mreža zmotila glede na pričakovan izhod. Tako dobimo napako s pomočjo katere lahko  vrednosti na utežeh popravimo tako, da minimaliziramo dobljeno napako. To počnemo od izhodnega sloja nevronov proti vhodnemu(od tod tudi ime -  vzvratno razširjanje napake oziroma backpropragation v anglrščini). Postopek ponavljamo na ostalih testnih primerih dokler se uteži ne ustalijo. Takrat se učenje konča.

\subsection{Vzvratno razširjanje napake}


Vzemimo splošen večsojen perceptron z $X_{N_X}$ nevroni v vhodnem sloju in $Y_{N_Y}$ nevroni v izhodnem sloju. Nevronska mreža naj sestoji iz $m$ skritih slojev, $m>0$, vsak skriti sloj pa naj vsabuje $N_k$ nevronov, kjer velja $1\leq k \leq m$. Nevrone v skritih slojih označimo sledeče: $H_{ij}$, kjer število $i$ ponazarja sloj v katerem se nevron nahaja, število $j$ pa zaporedno številko nevrona v tem sloju. 

Najprej za dani učni primer izračunamo izhodne vrednosti, tako da na vsakem nevronu uporabimo funkcijo aktivacije in kobinacije. Za $i$-ti nevron v prvem skritem sloju lahko zapišemo:

\[ H_{1i}(l) = f\left( \sum^{N_X}_{j=1}{W^{(1)}_{ij}X_j(l)}\right)  \]

Zaradi poenostavitve označimo še:

\[A{1i}(l) =\sum^{N_X}_{j=1}{W^{(1)}_{ij}X_j(l)} \]

Za poljubni $k$-ti sktiti sloj, kjer je $1\leq k \leq m$ ,tako zapišemo:

\[ H_{ki}(l) = f(A_{ki}(l)), \] 

kjer je

\[A_{ki}(l) = \sum^{N_{k-1}}_{j=1}{W^{(k)}_{ij}H_{k-1,j}(l)}.\]

V zgornji enačbi $H_{k-1,j}$ označuje že izračunane vrednosti nevronov v predhodnem sloju.
 
 V izhodnem sloju dobimo vrednosti, ki nas najbolj zanimajo. Označimo:

\[  Y_{i}(l) = f(A_{i}(l)) \quad \text{in} \quad A_{i}(l) = \sum^{N_{m}}_{j=1}{W^{(m+1)}_{ij}H_{m,j}(l)}.   \]

Tako smo dobili izhodne vrednosti, ki jih je izračunala naša mreža. Vrenosti primerjamo s testimi, ki bi si jih želeli dobiti za dani $l$-ti učni primer. Za posamezen nevron napako ozančimo sledeče:

\[e_i(l) = d_i(l) - Y_i(l), \]

lahko pa enačbo zapišemo tudi v vektoski obliki:

\[e(l) = d(l) - Y(l).\]

Napako celotne nevronske mreže za $l$-ti učni primer pa definiramo sledeče:

\begin{equation}
E(l) = \frac{1}{2}\sum^{N_Y}_{i=1}e_i^2(l)
\label{napaka}
\end{equation}

Uteži popravimo v smeri negativnega odvoda napake $E$, saj želimo napako minimalizirati. Tu gre za gradientno iskanje. Nove uteži izračunamo po sledeči formuli:

\begin{equation}
W^{NOV} = W - \eta \frac{dE}{dW}, 
\end{equation}

kjer $\eta$ označuje stopnjo učenja. 
Pri izračunu zgornjega odvoda si bomo pomagali s verižnim pravilom za odvajanje. Odvod enostavno izračunamo za izhodni sloj. Problem se pojavi pri popravljanju uteži v skritih slojih, saj uteži niso direktno povezane z napako. Kot bomo videli v nadaljevanju, je tudi tu rešitev verižno pravilo.

Najprej razpišimo formulo \eqref{napaka}, kjer upoštevamo formuli za $e_i$ in $Y_i$:

\[ 
\begin{aligned}
E(l) &= \frac{1}{2}\sum^{N_Y}_{i=1}e_i^2(l) =   \frac{1}{2}\sum^{N_Y}_{i=1}{(d_i(l) - Y_i(l))}^2(l) \\
&= \frac{1}{2}\sum^{N_Y}_{i=1}{(d_i(l)-f(\sum^{N_m}_{j=1}{W^{(m+1)}_{ij}H_{m,j}(l)))}}^2(l)  
\end{aligned}
\]

 Sedaj za izračun odvoda uporabimo verižno pravilo,  formulo razpišemo na posamezno utež:

\begin{equation}
 \frac{dE(l)}{dW_{ij}} = 
\end{equation}


\subsection{Lastnosti vzvratnega razširjanja napake}
Kljub temu, da imajo nevronske mreže veliko pozitivnih lastnosti, ima pravilo vzvratnega razširjanja napake tudi nekaj pomanjkljivosti.



\section{Funkcijski programski jezik OCaml}

\section{Praktični del}

\section{Zaključek}

\section*{Slovar strokovnih izrazov}

\geslo{}{}
\geslo{}{}

% seznam uporabljene literature
\begin{thebibliography}{99}

\bibitem{https://www.researchgate.net/publication/266396438_A_Gentle_Introduction_to_Backpropagation}

%\bibitem{}

\end{thebibliography}

\end{document}

